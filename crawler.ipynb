{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "s=Service(ChromeDriverManager().install())\n",
    "browser = webdriver.Chrome(service=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e8721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_main():\n",
    "    \n",
    "    papers_dic = {}\n",
    "    \n",
    "    url1 = 'https://www.sciencedirect.com/search?pub=Information%20Processing%20%26%20Management&date=2019-2022&accessTypes=openaccess&show=100'\n",
    "    url2 = 'https://www.sciencedirect.com/search?pub=Pattern%20Recognition&date=2019-2022&articleTypes=REV%2CFLA&accessTypes=openaccess&publicationTitles=272206&show=100'\n",
    "    url3 = 'https://www.sciencedirect.com/search?pub=Knowledge-Based%20Systems&date=2019-2022&articleTypes=FLA&accessTypes=openaccess&show=100'\n",
    "    \n",
    "    papers_dic ['Information Processing and Management'] = url1\n",
    "    papers_dic ['Pattern Recognition'] = url2\n",
    "    papers_dic ['knowledge based systems'] = url3\n",
    "\n",
    "    for key in papers_dic:\n",
    "        \n",
    "        url_link = papers_dic[key]\n",
    "        \n",
    "        data = [] \n",
    "\n",
    "        continue_crawl = True\n",
    "\n",
    "        while continue_crawl:\n",
    "            browser.get(url_link)\n",
    "            time.sleep(5)\n",
    "            html = browser.page_source\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            continue_crawl = False\n",
    "\n",
    "            for link in  soup.findAll(\"a\", {\"class\": \"anchor result-list-title-link u-font-serif text-s anchor-default\"}):\n",
    "                paper_link = 'https://www.sciencedirect.com/' + link.get('href')\n",
    "                browser.get(paper_link)\n",
    "                time.sleep(2)\n",
    "                html = browser.page_source\n",
    "                soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "                paper_details = []\n",
    "\n",
    "                title = soup.find(\"span\", {\"class\": \"title-text\"}).text\n",
    "                abstract = soup.find(\"div\",{\"class\":\"abstract author\"}).find(\"p\").text\n",
    "\n",
    "                authors_list = []\n",
    "                for author in soup.findAll(\"span\",{\"class\":\"content\"}):\n",
    "                    try:\n",
    "                        author_name = author.find(\"span\",{\"class\":\"text given-name\"}).text + ' ' + author.find(\"span\",{\"class\":\"text surname\"}).text\n",
    "                        authors_list.append(author_name)\n",
    "                    except:\n",
    "                        pass\n",
    "                authors = ', '.join(authors_list)\n",
    "\n",
    "\n",
    "                keywords_list = []\n",
    "                try:\n",
    "                    keyword_section = soup.findAll('div',{'class':\"keywords-section\"})\n",
    "\n",
    "                    if len(keyword_section) == 1:\n",
    "                        for each in keyword_section[0].findAll('span'):\n",
    "                            keywords_list.append(each.text)\n",
    "                    else:\n",
    "                        for each in soup.findAll('div',{'class':\"keywords-section\"})[1].findAll('span'):\n",
    "                            keywords_list.append(each.text)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                keywords = ', '.join(keywords_list)\n",
    "\n",
    "                year = soup.find(\"div\", {\"class\": \"text-xs\"}).text.split()[-2][:-1]\n",
    "                try:\n",
    "                    a = int(year)\n",
    "                except:\n",
    "                    if year == 'Page':\n",
    "                        year = soup.find(\"div\", {\"class\": \"text-xs\"}).text.split()[4][:-1]\n",
    "                        try:\n",
    "                            a = int(year)\n",
    "                        except:\n",
    "                            year = soup.find(\"div\", {\"class\": \"text-xs\"}).text.split()[5][:-1]\n",
    "                            try:\n",
    "                                a = int(year)\n",
    "                            except:\n",
    "                                year = soup.find(\"div\", {\"class\": \"text-xs\"}).text.split()[-3][:-1]              \n",
    "\n",
    "                paper_details.extend([title, abstract, authors, keywords, year])\n",
    "                data.append(paper_details)\n",
    "                \n",
    "                break\n",
    "\n",
    "        df=pd.DataFrame(data, columns=['title', 'abstract', 'authors', 'keywords', 'year'])\n",
    "        df['journal'] = key\n",
    "        df.to_csv(key+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
